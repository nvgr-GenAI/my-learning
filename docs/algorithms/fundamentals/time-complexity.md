# Time & Space Complexity Analysis ğŸ“Š

Understanding algorithmic complexity is fundamental to writing efficient code. This guide explains how to analyze and optimize the runtime and memory usage of your algorithms.

!!! tip "Quick Reference"
    | Complexity | Name | Speed for large inputs | Example |
    |------------|------|------------------------|---------|
    | **O(1)** | Constant | âš¡ Instant | Array access |
    | **O(log n)** | Logarithmic | ğŸš€ Super Fast | Binary search |
    | **O(n)** | Linear | ğŸš— Fast | Linear search |
    | **O(n log n)** | Linearithmic | ğŸš² Moderate | Merge sort |
    | **O(nÂ²)** | Quadratic | ğŸš¶ Slow | Bubble sort |
    | **O(2â¿)** | Exponential | ğŸ¢ Very slow | Recursive Fibonacci |

## Big O Notation Explained

Big O notation describes the performance or complexity of an algorithm in terms of:

- **Time complexity**: How runtime grows as input size increases
- **Space complexity**: How memory usage grows as input size increases

!!! info "Remember"
    Big O is concerned with the **rate of growth** as input size increases, not the exact number of operations.

## Complexity Classes

!!! abstract "Big O Cheat Sheet"
    === "Efficient Algorithms"
        | Complexity | Name | Performance | Example 1 | Example 2 | Example 3 |
        |------------|------|-------------|-----------|-----------|-----------|
        | **O(1)** | Constant | ğŸŸ¢ Excellent | Array access | Hash lookup | Stack push/pop |
        | **O(log n)** | Logarithmic | ğŸŸ¢ Excellent | Binary search | BST operations | Heap operations |
        | **O(n)** | Linear | ğŸŸ¡ Good | Linear search | Array traversal | Counting elements |
        | **O(n log n)** | Linearithmic | ğŸŸ¡ Good | Merge sort | Heap sort | Quick sort (avg) |

    === "Inefficient Algorithms"
        | Complexity | Name | Performance | Example 1 | Example 2 | Example 3 |
        |------------|------|-------------|-----------|-----------|-----------|
        | **O(nÂ²)** | Quadratic | ğŸ”´ Poor | Bubble sort | Selection sort | Nested loops |
        | **O(nÂ³)** | Cubic | ğŸ”´ Very Poor | Matrix multiply | Floyd-Warshall | Triple nested loops |
        | **O(2â¿)** | Exponential | âš« Terrible | Recursive Fibonacci | Subset generation | Tower of Hanoi |
        | **O(n!)** | Factorial | âš« Impossible | Permutations | Traveling Salesman | NP-complete problems |

## Visual Complexity Comparison

!!! example "Relative Growth Rates"
    ```
    O(1) < O(log n) < O(n) < O(n log n) < O(nÂ²) < O(nÂ³) < O(2â¿) < O(n!)
    ```

    | Input Size | O(1) | O(log n) | O(n) | O(n log n) | O(nÂ²) | O(nÂ³) | O(2â¿) | O(n!) |
    |------------|------|----------|------|------------|-------|-------|-------|-------|
    | **n=10**   | 1    | 3.3      | 10   | 33         | 100   | 1,000  | 1,024  | 3.6M  |
    | **n=100**  | 1    | 6.6      | 100  | 664        | 10K   | 1M     | 10^30  | 10^158 |
    | **n=1000** | 1    | 10       | 1K   | 10K        | 1M    | 1B     | 10^301 | 10^2568 |

    â° **Runtime Translation**:
    
    - **n = 10**: All algorithms finish quickly
    - **n = 100**: O(nÂ²) starts to slow down significantly 
    - **n = 1,000**: O(nÂ³) becomes impractical
    - **n = 1,000,000**: Only O(1), O(log n), O(n) remain feasible

## Common Data Structure Operations

!!! info "Data Structure Complexity Cheat Sheet"
    === "Arrays & Lists"
        | Operation | Array | Dynamic Array | Singly-Linked List | Doubly-Linked List |
        |-----------|-------|---------------|-------------------|-------------------|
        | **Access** | O(1) ğŸŸ¢ | O(1) ğŸŸ¢ | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ |
        | **Search** | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ |
        | **Insertion (beginning)** | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(1) ğŸŸ¢ | O(1) ğŸŸ¢ |
        | **Insertion (end)** | O(1)\* ğŸŸ¢ | O(1)\*\* ğŸŸ¢ | O(n)\*\*\* ğŸŸ¡ | O(1)\*\*\* ğŸŸ¢ |
        | **Insertion (middle)** | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ |
        | **Deletion (beginning)** | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(1) ğŸŸ¢ | O(1) ğŸŸ¢ |
        | **Deletion (end)** | O(1) ğŸŸ¢ | O(1) ğŸŸ¢ | O(n) ğŸŸ¡ | O(1) ğŸŸ¢ |
        | **Deletion (middle)** | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ |

        *If size is known, **Amortized, ***With tail pointer
    
    === "Trees & Hash Tables"
        | Operation | BST (balanced) | BST (unbalanced) | Hash Table |
        |-----------|----------------|------------------|------------|
        | **Access** | O(log n) ğŸŸ¢ | O(n) ğŸŸ¡ | N/A |
        | **Search** | O(log n) ğŸŸ¢ | O(n) ğŸŸ¡ | O(1)* ğŸŸ¢ |
        | **Insertion** | O(log n) ğŸŸ¢ | O(n) ğŸŸ¡ | O(1)* ğŸŸ¢ |
        | **Deletion** | O(log n) ğŸŸ¢ | O(n) ğŸŸ¡ | O(1)* ğŸŸ¢ |

        *Assuming good hash function with minimal collisions
    
    === "Heaps & Tries"
        | Operation | Min/Max Heap | Priority Queue | Trie |
        |-----------|--------------|----------------|------|
        | **Find Min/Max** | O(1) ğŸŸ¢ | O(1) ğŸŸ¢ | N/A |
        | **Insert** | O(log n) ğŸŸ¢ | O(log n) ğŸŸ¢ | O(m)* ğŸŸ¢ |
        | **Delete** | O(log n) ğŸŸ¢ | O(log n) ğŸŸ¢ | O(m)* ğŸŸ¢ |
        | **Search** | O(n) ğŸŸ¡ | O(n) ğŸŸ¡ | O(m)* ğŸŸ¢ |
        | **Prefix Search** | N/A | N/A | O(m)* ğŸŸ¢ |

        *Where m is the key length

## Sorting Algorithm Complexities

!!! tip "Sorting Algorithm Selection Guide"
    === "Comparison-Based"
        | Algorithm | Time (Best) | Time (Avg) | Time (Worst) | Space | Stable | When to Use |
        |-----------|-------------|------------|--------------|-------|--------|-------------|
        | **Bubble Sort** | O(n) | O(nÂ²) | O(nÂ²) | O(1) | âœ… | Small datasets, nearly sorted data |
        | **Selection Sort** | O(nÂ²) | O(nÂ²) | O(nÂ²) | O(1) | âŒ | Small datasets, minimizing swaps |
        | **Insertion Sort** | O(n) | O(nÂ²) | O(nÂ²) | O(1) | âœ… | Small datasets, online sorting |
        | **Merge Sort** | O(n log n) | O(n log n) | O(n log n) | O(n) | âœ… | Stable sorting required, linked lists |
        | **Quick Sort** | O(n log n) | O(n log n) | O(nÂ²) | O(log n) | âŒ | General purpose, average case performance |
        | **Heap Sort** | O(n log n) | O(n log n) | O(n log n) | O(1) | âŒ | Limited memory, guaranteed performance |

    === "Non-Comparison"
        | Algorithm | Time (Best) | Time (Avg) | Time (Worst) | Space | Stable | When to Use |
        |-----------|-------------|------------|--------------|-------|--------|-------------|
        | **Counting Sort** | O(n+k) | O(n+k) | O(n+k) | O(n+k) | âœ… | Small range of integers |
        | **Radix Sort** | O(nk) | O(nk) | O(nk) | O(n+k) | âœ… | Fixed-length integers or strings |
        | **Bucket Sort** | O(n+k) | O(n+k) | O(nÂ²) | O(n+k) | âœ… | Uniformly distributed data |

    Where n is input size, k is range of values

## Search Algorithm Complexities

!!! example "Search Algorithms Compared"
    | Algorithm | Time (Best) | Time (Avg) | Time (Worst) | Space | Use Case |
    |-----------|-------------|------------|--------------|-------|----------|
    | **Linear Search** | O(1) | O(n) | O(n) | O(1) | Unsorted data, small datasets |
    | **Binary Search** | O(1) | O(log n) | O(log n) | O(1) | Sorted arrays |
    | **Jump Search** | O(1) | O(âˆšn) | O(âˆšn) | O(1) | Sorted arrays, better than linear but simpler than binary |
    | **Interpolation Search** | O(1) | O(log log n) | O(n) | O(1) | Sorted, uniformly distributed arrays |

    **Graph Search**

    | Algorithm | Time | Space | Complete | Optimal | Use Case |
    |-----------|------|-------|----------|---------|----------|
    | **Depth-First Search** | O(V+E) | O(V) | âœ… | âŒ | Maze solving, topological sorting, cycle detection |
    | **Breadth-First Search** | O(V+E) | O(V) | âœ… | âœ…* | Shortest path (unweighted), connected components |
    | **Dijkstra's Algorithm** | O((V+E)log V) | O(V) | âœ… | âœ…** | Shortest path (non-negative weights) |
    | **A* Search** | O(E) | O(V) | âœ… | âœ…** | Shortest path with heuristic guidance |

    \* For unweighted graphs only  
    \** For graphs with non-negative edge weights  
    Where V = number of vertices, E = number of edges

## Common Trade-offs & Optimizations

!!! info "Space-Time Tradeoffs"
    | Technique | Time Benefit | Space Cost | Example |
    |-----------|-------------|------------|---------|
    | **Hash Tables** | O(n) â†’ O(1) lookup | O(1) â†’ O(n) space | Dictionary implementation |
    | **Memoization** | Exponential â†’ Polynomial | O(1) â†’ O(n) or more | Dynamic programming |
    | **Precomputation** | Runtime â†’ Compile time | Increased storage | Lookup tables, precomputed results |
    | **Index/Cache** | O(n) â†’ O(1) or O(log n) | Extra storage structures | Database indices, web caching |

## Analyzing Algorithms

!!! tip "Multiple Perspectives"
    === "Analysis Types"
        | Analysis Type | Description | When to Use |
        |---------------|-------------|------------|
        | **Best Case** | Performance under optimal conditions | Rarely useful except for theoretical understanding |
        | **Average Case** | Expected performance under normal conditions | Most practical for everyday use cases |
        | **Worst Case** | Performance under the most unfavorable conditions | Critical for reliability guarantees |
        | **Amortized** | Average cost over many operations | Data structures with occasional expensive operations |
    
    === "Advanced Concepts"
        | Concept | Description | Example |
        |---------|-------------|---------|
        | **Loop Invariants** | Conditions that remain true throughout loop execution | Binary search correctness proof |
        | **Recurrence Relations** | Equations describing recursive algorithm complexity | Master theorem applications |
        | **Asymptotic Notation** | O, Î©, and Î˜ notation for algorithm growth rates | Comparing algorithm efficiency |
        | **NP-Completeness** | Problems for which no known polynomial solution exists | Traveling Salesman Problem |

## Optimization Techniques

!!! success "Algorithm Improvement Strategies"
    | Category | Key Techniques | Examples |
    |----------|------------|----------|
    | **Algorithm Choice** | Choose appropriate complexity for your constraints | Replace bubble sort with quick sort |
    | **Data Structures** | Select optimal structures for your access patterns | HashMap instead of linear search |
    | **Code Optimization** | Use early termination and avoid redundant work | Break when found; lazy evaluation |
    | **Memory Management** | Implement caching and reduce allocations | Object pooling; result memoization |
    | **Concurrency** | Utilize parallelization where appropriate | Multi-threaded sorting; async I/O |

    **Additional Techniques**:
    
    1. **Algorithmic**: Dynamic programming, greedy algorithms, divide and conquer
    2. **Data Structure**: Using specialized structures (Bloom filters, tries, etc.)
    3. **System-Level**: I/O batching, memory mapping, locality optimization

## Real-World Considerations

!!! warning "Beyond Big O"
    | Consideration | Description |
    |---------------|-------------|
    | **Constant Factors** | For small inputs, O(nÂ²) with tiny constants might outperform O(n log n) with large constants |
    | **Cache Performance** | Algorithms with good locality often perform better in practice |
    | **Memory Hierarchy** | Consider CPU cache misses, page faults, and disk access |
    | **Input Distribution** | Some algorithms excel with specific data patterns |
    | **Hardware Architecture** | SIMD, multi-core, GPU acceleration opportunities |

## Practical Advice

!!! quote "Engineering Wisdom"
    > "Premature optimization is the root of all evil" â€” Donald Knuth

    1. **Make it work first**, then make it fast (if needed)
    2. **Measure before optimizing** â€” use profilers to identify bottlenecks
    3. **Focus on hot spots** â€” 80% of time is often spent in 20% of code
    4. **Consider maintenance** â€” sometimes readability trumps small performance gains
    5. **Know your constraints** â€” are you optimizing for speed, memory, or something else?

---

## Quick Decision Guide

!!! tip "Algorithm Selection Reference"
    | If you need to... | Consider using... | Avoid... |
    |-------------------|-------------------|----------|
    | **Search sorted data** | Binary search O(log n) | Linear search O(n) |
    | **Sort small datasets** | Insertion sort | Quick/merge sort (overhead) |
    | **Sort large datasets** | Quick sort, merge sort | Bubble, insertion sort |
    | **Queue with priority** | Heap/priority queue | Regular queue + resorting |
    | **Frequent lookups** | Hash table | Array/list searches |
    | **Ordered map operations** | Balanced BST | Hash tables |
    | **Prefix matching** | Trie | Linear string searches |
    | **Shortest path in graph** | Dijkstra's/A* | DFS (not optimal) |

Remember that the best algorithm is often the one that's good enough for your specific requirements and constraints, not necessarily the one with the best theoretical complexity.
