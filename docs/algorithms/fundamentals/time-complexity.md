# Time & Space Complexity Analysis

Understanding algorithmic complexity is fundamental to writing efficient code. This guide covers how to analyze and optimize runtime and memory usage.

!!! tip "Advanced Topics"
    Looking for deeper analysis? Check out [Advanced Complexity Analysis](complexity-analysis-advanced.md) covering:

    - **Analyzing Recursive Algorithms** (Master Theorem, recursion trees)
    - **Amortized Analysis** (dynamic arrays, union-find)
    - **Space Complexity Deep Dive** (call stack, in-place algorithms)
    - **Hidden Complexity Traps** (string concatenation, nested library calls)
    - **Interview Communication** (how to explain complexity)
    - **Code Examples** by complexity class

---

## ðŸ“Š Complexity Fundamentals

=== "Big O Basics"
    **What is Big O?** Describes how runtime/memory grows as input size increases.

    **Key Principle:** Focus on **rate of growth**, not exact operations.

    **Growth Order:** O(1) < O(log n) < O(n) < O(n log n) < O(nÂ²) < O(nÂ³) < O(2â¿) < O(n!)

    | Complexity | Name | Performance | Example Operations |
    |------------|------|-------------|-------------------|
    | O(1) | Constant | ðŸŸ¢ Instant | Array access, hash lookup, stack push/pop |
    | O(log n) | Logarithmic | ðŸŸ¢ Super Fast | Binary search, BST operations |
    | O(n) | Linear | ðŸŸ¡ Fast | Linear search, array traversal |
    | O(n log n) | Linearithmic | ðŸŸ¡ Moderate | Merge sort, heap sort |
    | O(nÂ²) | Quadratic | ðŸ”´ Slow | Bubble sort, nested loops |
    | O(nÂ³) | Cubic | ðŸ”´ Very Slow | Matrix multiply, triple nested loops |
    | O(2â¿) | Exponential | âš« Terrible | Recursive Fibonacci, subsets |
    | O(n!) | Factorial | âš« Impossible | Permutations, TSP |

=== "Growth Comparison"
    **Relative Performance by Input Size:**

    | Input Size | O(1) | O(log n) | O(n) | O(n log n) | O(nÂ²) | O(2â¿) | O(n!) |
    |------------|------|----------|------|------------|-------|-------|-------|
    | n=10 | 1 | 3 | 10 | 33 | 100 | 1K | 3.6M |
    | n=100 | 1 | 7 | 100 | 664 | 10K | 10Â³â° | 10Â¹âµâ¸ |
    | n=1,000 | 1 | 10 | 1K | 10K | 1M | 10Â³â°Â¹ | 10Â²âµâ¶â¸ |

    **â° Practical Limits:**
    - **n=10:** All algorithms acceptable
    - **n=100:** O(nÂ²) starts slowing down
    - **n=1,000:** O(nÂ³) becomes impractical
    - **n=1M:** Only O(1), O(log n), O(n) feasible

=== "Analysis Types"
    | Type | Description | When to Use |
    |------|-------------|-------------|
    | **Best Case** | Optimal conditions | Theoretical understanding |
    | **Average Case** | Normal conditions | Practical everyday use |
    | **Worst Case** | Most unfavorable | Reliability guarantees |
    | **Amortized** | Average over many ops | Occasional expensive operations |

---

## ðŸ—‚ï¸ Data Structures Complexity

=== "Arrays & Lists"
    | Operation | Array | Dynamic Array | Linked List | Doubly-Linked |
    |-----------|-------|---------------|-------------|---------------|
    | Access | O(1) ðŸŸ¢ | O(1) ðŸŸ¢ | O(n) ðŸŸ¡ | O(n) ðŸŸ¡ |
    | Search | O(n) ðŸŸ¡ | O(n) ðŸŸ¡ | O(n) ðŸŸ¡ | O(n) ðŸŸ¡ |
    | Insert (start) | O(n) ðŸŸ¡ | O(n) ðŸŸ¡ | O(1) ðŸŸ¢ | O(1) ðŸŸ¢ |
    | Insert (end) | O(1)* ðŸŸ¢ | O(1)** ðŸŸ¢ | O(n) ðŸŸ¡ | O(1)*** ðŸŸ¢ |
    | Delete (start) | O(n) ðŸŸ¡ | O(n) ðŸŸ¡ | O(1) ðŸŸ¢ | O(1) ðŸŸ¢ |
    | Delete (end) | O(1) ðŸŸ¢ | O(1) ðŸŸ¢ | O(n) ðŸŸ¡ | O(1) ðŸŸ¢ |

    *If size known | **Amortized | ***With tail pointer

=== "Trees & Hash Tables"
    | Operation | BST (Balanced) | BST (Unbalanced) | Hash Table | AVL/Red-Black |
    |-----------|----------------|------------------|------------|---------------|
    | Access | O(log n) ðŸŸ¢ | O(n) ðŸŸ¡ | N/A | O(log n) ðŸŸ¢ |
    | Search | O(log n) ðŸŸ¢ | O(n) ðŸŸ¡ | O(1)* ðŸŸ¢ | O(log n) ðŸŸ¢ |
    | Insert | O(log n) ðŸŸ¢ | O(n) ðŸŸ¡ | O(1)* ðŸŸ¢ | O(log n) ðŸŸ¢ |
    | Delete | O(log n) ðŸŸ¢ | O(n) ðŸŸ¡ | O(1)* ðŸŸ¢ | O(log n) ðŸŸ¢ |

    *Average case with good hash function

=== "Heaps & Tries"
    | Operation | Min/Max Heap | Priority Queue | Trie |
    |-----------|--------------|----------------|------|
    | Find Min/Max | O(1) ðŸŸ¢ | O(1) ðŸŸ¢ | N/A |
    | Insert | O(log n) ðŸŸ¢ | O(log n) ðŸŸ¢ | O(m)* ðŸŸ¢ |
    | Delete | O(log n) ðŸŸ¢ | O(log n) ðŸŸ¢ | O(m)* ðŸŸ¢ |
    | Search | O(n) ðŸŸ¡ | O(n) ðŸŸ¡ | O(m)* ðŸŸ¢ |
    | Prefix Search | N/A | N/A | O(m)* ðŸŸ¢ |

    *Where m = key length

=== "Graphs"
    | Representation | Space | Add Vertex | Add Edge | Remove Vertex | Remove Edge | Query Edge |
    |----------------|-------|------------|----------|---------------|-------------|------------|
    | Adjacency List | O(V+E) | O(1) | O(1) | O(V+E) | O(E) | O(V) |
    | Adjacency Matrix | O(VÂ²) | O(VÂ²) | O(1) | O(VÂ²) | O(1) | O(1) |

    **Graph Algorithms:**

    | Algorithm | Time | Space | Optimal | Use Case |
    |-----------|------|-------|---------|----------|
    | DFS | O(V+E) | O(V) | âŒ | Topological sort, cycle detection |
    | BFS | O(V+E) | O(V) | âœ…* | Shortest path (unweighted) |
    | Dijkstra | O((V+E)log V) | O(V) | âœ…** | Shortest path (non-negative weights) |
    | Bellman-Ford | O(VE) | O(V) | âœ…*** | Shortest path (negative weights) |
    | A* | O(E) | O(V) | âœ…** | Heuristic-guided shortest path |

    *Unweighted graphs | **Non-negative weights | ***Can detect negative cycles

---

## ðŸ”„ Algorithms Complexity

=== "Sorting Algorithms"
    **Comparison-Based:**

    | Algorithm | Best | Average | Worst | Space | Stable | Use Case |
    |-----------|------|---------|-------|-------|--------|----------|
    | Bubble Sort | O(n) | O(nÂ²) | O(nÂ²) | O(1) | âœ… | Nearly sorted, small data |
    | Insertion Sort | O(n) | O(nÂ²) | O(nÂ²) | O(1) | âœ… | Small data, online sorting |
    | Selection Sort | O(nÂ²) | O(nÂ²) | O(nÂ²) | O(1) | âŒ | Minimize swaps |
    | Merge Sort | O(n log n) | O(n log n) | O(n log n) | O(n) | âœ… | Stable sorting, linked lists |
    | Quick Sort | O(n log n) | O(n log n) | O(nÂ²) | O(log n) | âŒ | General purpose (best avg) |
    | Heap Sort | O(n log n) | O(n log n) | O(n log n) | O(1) | âŒ | Guaranteed performance |

    **Non-Comparison:**

    | Algorithm | Time | Space | Stable | Use Case |
    |-----------|------|-------|--------|----------|
    | Counting Sort | O(n+k) | O(n+k) | âœ… | Small integer range |
    | Radix Sort | O(nk) | O(n+k) | âœ… | Fixed-length integers/strings |
    | Bucket Sort | O(n+k) | O(n+k) | âœ… | Uniformly distributed data |

=== "Search Algorithms"
    **Array Search:**

    | Algorithm | Best | Average | Worst | Space | Requirement |
    |-----------|------|---------|-------|-------|-------------|
    | Linear Search | O(1) | O(n) | O(n) | O(1) | None |
    | Binary Search | O(1) | O(log n) | O(log n) | O(1) | Sorted array |
    | Jump Search | O(1) | O(âˆšn) | O(âˆšn) | O(1) | Sorted array |
    | Interpolation | O(1) | O(log log n) | O(n) | O(1) | Sorted, uniform distribution |

    **String Search:**

    | Algorithm | Preprocessing | Search | Space | Use Case |
    |-----------|--------------|--------|-------|----------|
    | Naive | O(1) | O(mn) | O(1) | Simple, short patterns |
    | KMP | O(m) | O(n) | O(m) | Pattern matching |
    | Boyer-Moore | O(m+k) | O(n/m) best | O(k) | Large alphabets |
    | Rabin-Karp | O(m) | O(n+m) | O(1) | Multiple pattern search |

=== "Dynamic Programming"
    | Problem Type | Time | Space | Optimization Technique |
    |--------------|------|-------|----------------------|
    | Fibonacci | O(n) | O(n) â†’ O(1) | Space optimization |
    | Longest Common Subsequence | O(mn) | O(mn) â†’ O(min(m,n)) | Rolling array |
    | Knapsack (0/1) | O(nW) | O(nW) â†’ O(W) | 1D DP |
    | Matrix Chain Multiplication | O(nÂ³) | O(nÂ²) | Memoization |
    | Edit Distance | O(mn) | O(mn) â†’ O(min(m,n)) | Space optimization |

---

## âš¡ Optimization Strategies

=== "Time-Space Tradeoffs"
    | Technique | Time Gain | Space Cost | Example |
    |-----------|-----------|------------|---------|
    | Hash Tables | O(n) â†’ O(1) | +O(n) | Two-sum problem |
    | Memoization | Exponential â†’ Polynomial | +O(n) or more | DP problems |
    | Precomputation | Runtime â†’ Compile time | +Storage | Lookup tables |
    | Indexing | O(n) â†’ O(log n) or O(1) | +O(n) | Database indices |
    | Caching | Repeated â†’ O(1) | +O(cache size) | Web caching |

=== "Algorithm Improvement"
    **Key Techniques:**

    1. **Choose Better Algorithm:** Bubble sort O(nÂ²) â†’ Quick sort O(n log n)
    2. **Optimize Data Structure:** Linear search â†’ Hash table lookup
    3. **Early Termination:** Break when condition met
    4. **Avoid Redundant Work:** Cache results, avoid recalculation
    5. **Divide & Conquer:** Break into smaller subproblems
    6. **Two Pointers:** Reduce nested loops from O(nÂ²) to O(n)
    7. **Sliding Window:** Optimize subarray problems
    8. **Binary Search:** O(n) â†’ O(log n) on sorted data

=== "Real-World Factors"
    **Beyond Big O:**

    | Factor | Impact |
    |--------|--------|
    | **Constant Factors** | Small inputs: O(nÂ²) with tiny constants may beat O(n log n) |
    | **Cache Locality** | Sequential access faster than random access |
    | **Memory Hierarchy** | CPU cache > RAM > Disk (1x vs 100x vs 100,000x) |
    | **Input Distribution** | Quick sort excellent on random, poor on sorted |
    | **Hardware** | SIMD, multi-core, GPU opportunities |
    | **I/O Bounds** | Disk/network often bottleneck, not CPU |

---

## ðŸŽ¯ Quick Reference Guide

=== "Algorithm Selection"
    | Need | Use | Avoid | Complexity |
    |------|-----|-------|------------|
    | Sort small data (<50) | Insertion sort | Quick/merge sort | O(nÂ²) acceptable |
    | Sort large data | Quick/merge sort | Bubble/insertion | O(n log n) |
    | Search sorted data | Binary search | Linear search | O(log n) |
    | Frequent lookups | Hash table | Array search | O(1) |
    | Ordered iteration | BST | Hash table | O(log n) |
    | Priority processing | Heap | Sorted array | O(log n) |
    | Prefix matching | Trie | Linear string search | O(m) |
    | Shortest path | Dijkstra/A* | DFS | O((V+E)log V) |

=== "Data Structure Selection"
    | Problem Pattern | Data Structure | Why |
    |----------------|----------------|-----|
    | Fast access by key | Hash table | O(1) lookup |
    | Maintain sorted order | BST/AVL | O(log n) operations |
    | Find min/max frequently | Heap | O(1) peek, O(log n) insert/delete |
    | Prefix/suffix queries | Trie | O(m) string operations |
    | FIFO order | Queue | O(1) enqueue/dequeue |
    | LIFO order | Stack | O(1) push/pop |
    | Range queries | Segment tree | O(log n) query/update |
    | Dynamic median | Two heaps | O(log n) insert, O(1) median |

=== "Common Patterns"
    | Pattern | Complexity Reduction | Example |
    |---------|---------------------|---------|
    | Two Pointers | O(nÂ²) â†’ O(n) | Two sum on sorted array |
    | Sliding Window | O(nÂ²) â†’ O(n) | Max subarray of size k |
    | Binary Search | O(n) â†’ O(log n) | Search in rotated array |
    | Hash Map | O(nÂ²) â†’ O(n) | Two sum on unsorted array |
    | Prefix Sum | O(nÂ²) â†’ O(n) | Subarray sum queries |
    | Monotonic Stack | O(nÂ²) â†’ O(n) | Next greater element |
    | Union Find | O(nÂ²) â†’ O(nÂ·Î±(n)) | Connected components |

---

## ðŸ’¡ Best Practices

!!! success "Engineering Principles"
    1. **Make it work first, then optimize** - Correctness before performance
    2. **Measure before optimizing** - Use profilers to find bottlenecks
    3. **Focus on hot paths** - 80% time spent in 20% of code
    4. **Consider readability** - Maintainability often > minor performance gains
    5. **Know your constraints** - Optimize for speed, memory, or both based on needs
    6. **Start with simple** - Use O(nÂ²) if n<100 and it's clearer

!!! warning "Common Mistakes"
    - Premature optimization without profiling
    - Ignoring constant factors for small inputs
    - Over-engineering when simple solution suffices
    - Not considering average vs worst case
    - Forgetting space complexity
    - Optimizing already-fast code

!!! quote "Remember"
    > "Premature optimization is the root of all evil" â€” Donald Knuth

    The best algorithm is the one that meets your requirements while remaining maintainableâ€”not necessarily the one with optimal theoretical complexity.
